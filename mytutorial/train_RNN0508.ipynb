{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.5 实现 Char RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.5.4 《 训练模型与生成文字过程 》"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先来看简单的生成英文的例子。使用的训练文件 shakespeare.txt 放置在data/文件夹下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from read_utils import TextConverter, batch_generator\n",
    "from model import CharRNN\n",
    "import os\n",
    "import codecs\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "tf.flags.DEFINE_string('name', 'default', 'name of the model')\n",
    "tf.flags.DEFINE_integer('num_seqs', 100, 'number of seqs in one batch')\n",
    "tf.flags.DEFINE_integer('num_steps', 100, 'length of one seq')\n",
    "tf.flags.DEFINE_integer('lstm_size', 128, 'size of hidden state of lstm')\n",
    "tf.flags.DEFINE_integer('num_layers', 2, 'number of lstm layers')\n",
    "tf.flags.DEFINE_boolean('use_embedding', False, 'whether to use embedding')\n",
    "tf.flags.DEFINE_integer('embedding_size', 128, 'size of embedding')\n",
    "tf.flags.DEFINE_float('learning_rate', 0.001, 'learning_rate')\n",
    "tf.flags.DEFINE_float('train_keep_prob', 0.5, 'dropout rate during training')\n",
    "tf.flags.DEFINE_string('input_file', '', 'utf8 encoded text file')\n",
    "tf.flags.DEFINE_integer('max_steps', 100000, 'max steps to train')\n",
    "tf.flags.DEFINE_integer('save_every_n', 1000, 'save the model every n steps')\n",
    "tf.flags.DEFINE_integer('log_every_n', 10, 'log to the screen every n steps')\n",
    "tf.flags.DEFINE_integer('max_vocab', 3500, 'max char number')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、训练英文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'shakespeare', 'num_seqs': 32, 'num_steps': 50, 'lstm_size': 128, 'num_layers': 2, 'use_embedding': False, 'embedding_size': 128, 'learning_rate': 0.01, 'train_keep_prob': 0.5, 'input_file': 'data/shakespeare.txt', 'max_steps': 20000, 'save_every_n': 1000, 'log_every_n': 10, 'max_vocab': 3500}\n",
      "\n",
      "67\n",
      "68\n",
      "WARNING:tensorflow:From /home/eric/workspace/pyth_and_dl/pm_rnn/mytutorial/model.py:93: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "step: 10/20000...  loss: 3.4994...  0.1114 sec/batch\n",
      "step: 20/20000...  loss: 3.3146...  0.1127 sec/batch\n",
      "step: 30/20000...  loss: 3.2427...  0.1144 sec/batch\n",
      "step: 40/20000...  loss: 3.3620...  0.1240 sec/batch\n",
      "step: 50/20000...  loss: 3.2330...  0.1156 sec/batch\n",
      "step: 60/20000...  loss: 3.1845...  0.1293 sec/batch\n",
      "step: 70/20000...  loss: 3.1274...  0.1133 sec/batch\n",
      "step: 80/20000...  loss: 3.0524...  0.1138 sec/batch\n",
      "step: 90/20000...  loss: 2.9929...  0.1115 sec/batch\n",
      "step: 100/20000...  loss: 2.8626...  0.1128 sec/batch\n",
      "step: 110/20000...  loss: 2.7905...  0.1105 sec/batch\n",
      "step: 120/20000...  loss: 2.6353...  0.1123 sec/batch\n",
      "step: 130/20000...  loss: 2.6980...  0.1113 sec/batch\n",
      "step: 140/20000...  loss: 2.6154...  0.1107 sec/batch\n",
      "step: 150/20000...  loss: 2.5557...  0.1109 sec/batch\n",
      "step: 160/20000...  loss: 2.6420...  0.1104 sec/batch\n",
      "step: 170/20000...  loss: 2.5059...  0.1132 sec/batch\n",
      "step: 180/20000...  loss: 2.5882...  0.1152 sec/batch\n",
      "step: 190/20000...  loss: 2.5321...  0.1129 sec/batch\n",
      "step: 200/20000...  loss: 2.4865...  0.1113 sec/batch\n",
      "step: 210/20000...  loss: 2.4946...  0.1112 sec/batch\n",
      "step: 220/20000...  loss: 2.4883...  0.1119 sec/batch\n",
      "step: 230/20000...  loss: 2.4263...  0.1109 sec/batch\n",
      "step: 240/20000...  loss: 2.4294...  0.1104 sec/batch\n",
      "step: 250/20000...  loss: 2.4008...  0.1105 sec/batch\n",
      "step: 260/20000...  loss: 2.4281...  0.1104 sec/batch\n",
      "step: 270/20000...  loss: 2.3398...  0.1099 sec/batch\n",
      "step: 280/20000...  loss: 2.3874...  0.1110 sec/batch\n",
      "step: 290/20000...  loss: 2.3674...  0.1108 sec/batch\n",
      "step: 300/20000...  loss: 2.3487...  0.1145 sec/batch\n",
      "step: 310/20000...  loss: 2.2778...  0.1122 sec/batch\n",
      "step: 320/20000...  loss: 2.3138...  0.1122 sec/batch\n",
      "step: 330/20000...  loss: 2.3127...  0.1163 sec/batch\n",
      "step: 340/20000...  loss: 2.3338...  0.1104 sec/batch\n",
      "step: 350/20000...  loss: 2.3211...  0.1159 sec/batch\n",
      "step: 360/20000...  loss: 2.3120...  0.1155 sec/batch\n",
      "step: 370/20000...  loss: 2.3409...  0.1114 sec/batch\n",
      "step: 380/20000...  loss: 2.2764...  0.1252 sec/batch\n",
      "step: 390/20000...  loss: 2.3107...  0.1148 sec/batch\n",
      "step: 400/20000...  loss: 2.1943...  0.1180 sec/batch\n",
      "step: 410/20000...  loss: 2.2911...  0.1212 sec/batch\n",
      "step: 420/20000...  loss: 2.3291...  0.1120 sec/batch\n",
      "step: 430/20000...  loss: 2.2898...  0.1403 sec/batch\n",
      "step: 440/20000...  loss: 2.2793...  0.1133 sec/batch\n",
      "step: 450/20000...  loss: 2.2756...  0.1110 sec/batch\n",
      "step: 460/20000...  loss: 2.2406...  0.1104 sec/batch\n",
      "step: 470/20000...  loss: 2.2348...  0.1107 sec/batch\n",
      "step: 480/20000...  loss: 2.2272...  0.1103 sec/batch\n",
      "step: 490/20000...  loss: 2.2668...  0.1107 sec/batch\n",
      "step: 500/20000...  loss: 2.2251...  0.1100 sec/batch\n",
      "step: 510/20000...  loss: 2.2119...  0.1124 sec/batch\n",
      "step: 520/20000...  loss: 2.2713...  0.1121 sec/batch\n",
      "step: 530/20000...  loss: 2.2590...  0.1122 sec/batch\n",
      "step: 540/20000...  loss: 2.2182...  0.1102 sec/batch\n",
      "step: 550/20000...  loss: 2.1705...  0.1118 sec/batch\n",
      "step: 560/20000...  loss: 2.2020...  0.1095 sec/batch\n",
      "step: 570/20000...  loss: 2.1834...  0.1116 sec/batch\n",
      "step: 580/20000...  loss: 2.1891...  0.1141 sec/batch\n",
      "step: 590/20000...  loss: 2.1814...  0.1139 sec/batch\n",
      "step: 600/20000...  loss: 2.2363...  0.1104 sec/batch\n",
      "step: 610/20000...  loss: 2.1366...  0.1124 sec/batch\n",
      "step: 620/20000...  loss: 2.2383...  0.1278 sec/batch\n",
      "step: 630/20000...  loss: 2.2058...  0.1114 sec/batch\n",
      "step: 640/20000...  loss: 2.1307...  0.1199 sec/batch\n",
      "step: 650/20000...  loss: 2.2109...  0.1132 sec/batch\n",
      "step: 660/20000...  loss: 2.1580...  0.1152 sec/batch\n",
      "step: 670/20000...  loss: 2.1530...  0.1125 sec/batch\n",
      "step: 680/20000...  loss: 2.1320...  0.1103 sec/batch\n",
      "step: 690/20000...  loss: 2.1117...  0.1138 sec/batch\n",
      "step: 700/20000...  loss: 2.1738...  0.1105 sec/batch\n",
      "step: 710/20000...  loss: 2.1170...  0.1150 sec/batch\n",
      "step: 720/20000...  loss: 2.1364...  0.1123 sec/batch\n",
      "step: 730/20000...  loss: 2.1291...  0.1117 sec/batch\n",
      "step: 740/20000...  loss: 2.1223...  0.1161 sec/batch\n",
      "step: 750/20000...  loss: 2.1085...  0.1114 sec/batch\n",
      "step: 760/20000...  loss: 2.1228...  0.1264 sec/batch\n",
      "step: 770/20000...  loss: 2.0845...  0.1105 sec/batch\n",
      "step: 780/20000...  loss: 2.0911...  0.1203 sec/batch\n",
      "step: 790/20000...  loss: 2.1207...  0.1101 sec/batch\n",
      "step: 800/20000...  loss: 2.1036...  0.1247 sec/batch\n",
      "step: 810/20000...  loss: 2.1028...  0.1270 sec/batch\n",
      "step: 820/20000...  loss: 2.0565...  0.1139 sec/batch\n",
      "step: 830/20000...  loss: 2.0518...  0.1184 sec/batch\n",
      "step: 840/20000...  loss: 2.1407...  0.1153 sec/batch\n",
      "step: 850/20000...  loss: 2.0716...  0.1274 sec/batch\n",
      "step: 860/20000...  loss: 2.0942...  0.1283 sec/batch\n",
      "step: 870/20000...  loss: 2.1012...  0.1150 sec/batch\n",
      "step: 880/20000...  loss: 2.0707...  0.1186 sec/batch\n",
      "step: 890/20000...  loss: 2.1478...  0.1179 sec/batch\n",
      "step: 900/20000...  loss: 2.0913...  0.1446 sec/batch\n",
      "step: 910/20000...  loss: 2.1342...  0.1515 sec/batch\n",
      "step: 920/20000...  loss: 2.0814...  0.1559 sec/batch\n",
      "step: 930/20000...  loss: 2.0619...  0.1342 sec/batch\n",
      "step: 940/20000...  loss: 2.0275...  0.1167 sec/batch\n",
      "step: 950/20000...  loss: 2.0722...  0.1127 sec/batch\n",
      "step: 960/20000...  loss: 2.0229...  0.1125 sec/batch\n",
      "step: 970/20000...  loss: 2.0578...  0.1227 sec/batch\n",
      "step: 980/20000...  loss: 2.0669...  0.1223 sec/batch\n",
      "step: 990/20000...  loss: 2.0939...  0.1105 sec/batch\n",
      "step: 1000/20000...  loss: 2.0782...  0.1120 sec/batch\n",
      "step: 1010/20000...  loss: 2.0459...  0.1146 sec/batch\n",
      "step: 1020/20000...  loss: 2.0628...  0.1133 sec/batch\n",
      "step: 1030/20000...  loss: 2.0419...  0.1114 sec/batch\n",
      "step: 1040/20000...  loss: 2.0624...  0.1098 sec/batch\n",
      "step: 1050/20000...  loss: 2.0518...  0.1196 sec/batch\n",
      "step: 1060/20000...  loss: 2.0414...  0.1126 sec/batch\n",
      "step: 1070/20000...  loss: 2.0243...  0.1118 sec/batch\n",
      "step: 1080/20000...  loss: 1.9714...  0.1109 sec/batch\n",
      "step: 1090/20000...  loss: 2.0295...  0.1102 sec/batch\n",
      "step: 1100/20000...  loss: 1.9479...  0.1134 sec/batch\n",
      "step: 1110/20000...  loss: 2.0129...  0.1199 sec/batch\n",
      "step: 1120/20000...  loss: 2.0565...  0.1111 sec/batch\n",
      "step: 1130/20000...  loss: 1.9820...  0.1115 sec/batch\n",
      "step: 1140/20000...  loss: 2.0398...  0.1356 sec/batch\n",
      "step: 1150/20000...  loss: 2.0510...  0.1149 sec/batch\n",
      "step: 1160/20000...  loss: 2.0207...  0.1133 sec/batch\n",
      "step: 1170/20000...  loss: 2.0905...  0.1114 sec/batch\n",
      "step: 1180/20000...  loss: 2.0018...  0.1112 sec/batch\n",
      "step: 1190/20000...  loss: 1.9735...  0.1112 sec/batch\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3914101f1d0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/tensorflow/python/platform/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    124\u001b[0m   \u001b[0;31m# Call the main function, passing through any arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m   \u001b[0;31m# to the final program.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m   \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-3914101f1d0b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(_)\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_every_n\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_every_n\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m                 )\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/pyth_and_dl/pm_rnn/mytutorial/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, batch_generator, max_steps, save_path, save_every_n, log_every_n)\u001b[0m\n\u001b[1;32m    118\u001b[0m                                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                                                      self.optimizer],\n\u001b[0;32m--> 120\u001b[0;31m                                                     feed_dict=feed)\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                 \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main(_):\n",
    "    model_path = os.path.join('model', FLAGS.name)\n",
    "    if os.path.exists(model_path) is False:\n",
    "        os.makedirs(model_path)\n",
    "    with codecs.open(FLAGS.input_file, encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    converter = TextConverter(text, FLAGS.max_vocab)\n",
    "    converter.save_to_file(os.path.join(model_path, 'converter.pkl'))\n",
    "\n",
    "    arr = converter.text_to_arr(text)\n",
    "    g = batch_generator(arr, FLAGS.num_seqs, FLAGS.num_steps)\n",
    "    print(converter.vocab_size)\n",
    "    model = CharRNN(converter.vocab_size,\n",
    "                    num_seqs=FLAGS.num_seqs,\n",
    "                    num_steps=FLAGS.num_steps,\n",
    "                    lstm_size=FLAGS.lstm_size,\n",
    "                    num_layers=FLAGS.num_layers,\n",
    "                    learning_rate=FLAGS.learning_rate,\n",
    "                    train_keep_prob=FLAGS.train_keep_prob,\n",
    "                    use_embedding=FLAGS.use_embedding,\n",
    "                    embedding_size=FLAGS.embedding_size\n",
    "                    )\n",
    "    model.train(g,\n",
    "                FLAGS.max_steps,\n",
    "                model_path,\n",
    "                FLAGS.save_every_n,\n",
    "                FLAGS.log_every_n,\n",
    "                )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unknown_flags, unparsed_args = FLAGS._parse_args([\n",
    "        \"--input_file\",\"data/shakespeare.txt\",\n",
    "        \"--name\",\"shakespeare\",\n",
    "        \"--num_steps\",\"50\",\n",
    "        \"--num_seqs\",\"32\",\n",
    "        \"--learning_rate\",\"0.01\",\n",
    "        \"--max_steps\",\"20000\"\n",
    "    ],True)\n",
    "    mf = FLAGS.flag_values_dict()\n",
    "    print(mf)\n",
    "    print(\"\")\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 对应 python 文件的shell命令语句\n",
    "# python train.py \\\n",
    "#     --input_file data/shakespeare.txt \\\n",
    "#     --name shakespeare \\\n",
    "#     --num_steps 50 \\\n",
    "#     --num_seqs 32 \\\n",
    "#     --learning_rate 0.01 \\\n",
    "#     --max_steps 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2、训练中文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'poetry', 'num_seqs': 32, 'num_steps': 26, 'lstm_size': 128, 'num_layers': 2, 'use_embedding': True, 'embedding_size': 128, 'learning_rate': 0.005, 'train_keep_prob': 0.5, 'input_file': 'data/poetry.txt', 'max_steps': 10000, 'save_every_n': 1000, 'log_every_n': 10, 'max_vocab': 3500}\n",
      "\n",
      "5387\n",
      "3501\n",
      "WARNING:tensorflow:From /home/eric/workspace/pyth_and_dl/pm_rnn/mytutorial/model.py:93: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main(_):\n",
    "    model_path = os.path.join('model', FLAGS.name)\n",
    "    if os.path.exists(model_path) is False:\n",
    "        os.makedirs(model_path)\n",
    "    with codecs.open(FLAGS.input_file, encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    converter = TextConverter(text, FLAGS.max_vocab)\n",
    "    converter.save_to_file(os.path.join(model_path, 'converter.pkl'))\n",
    "\n",
    "    arr = converter.text_to_arr(text)\n",
    "    g = batch_generator(arr, FLAGS.num_seqs, FLAGS.num_steps)\n",
    "    print(converter.vocab_size)\n",
    "    model = CharRNN(converter.vocab_size,\n",
    "                    num_seqs=FLAGS.num_seqs,\n",
    "                    num_steps=FLAGS.num_steps,\n",
    "                    lstm_size=FLAGS.lstm_size,\n",
    "                    num_layers=FLAGS.num_layers,\n",
    "                    learning_rate=FLAGS.learning_rate,\n",
    "                    train_keep_prob=FLAGS.train_keep_prob,\n",
    "                    use_embedding=FLAGS.use_embedding,\n",
    "                    embedding_size=FLAGS.embedding_size\n",
    "                    )\n",
    "    model.train(g,\n",
    "                FLAGS.max_steps,\n",
    "                model_path,\n",
    "                FLAGS.save_every_n,\n",
    "                FLAGS.log_every_n,\n",
    "                )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unknown_flags, unparsed_args = FLAGS._parse_args([\n",
    "        \"--use_embedding\",\"\",\n",
    "        \"--input_file\",\"data/poetry.txt\",\n",
    "        \"--name\",\"poetry\",\n",
    "        \"--num_steps\",\"26\",\n",
    "        \"--num_seqs\",\"32\",\n",
    "        \"--learning_rate\",\"0.005\",\n",
    "        \"--max_steps\",\"10000\"\n",
    "    ],True)\n",
    "    mf = FLAGS.flag_values_dict()\n",
    "    print(mf)\n",
    "    print(\"\")\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 对应 python 文件的shell命令语句\n",
    "# python train.py \\\n",
    "#     --use_embedding \\\n",
    "#     --input_file data/poetry.txt \\\n",
    "#     --name poetry \\\n",
    "#     --num_steps 26 \\\n",
    "#     --num_seqs 32 \\\n",
    "#     --learning_rate 0.005 \\\n",
    "#     --max_steps 10000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu bin py3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
