{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实现RNN的基本单元——RNN Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Tensorflow中定义一个基本的RNN单元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "rnn_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=128)\n",
    "print(rnn_cell.state_size) # 打出state_size看一下，应为128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Tensorflow中定义一个基本的LSTM单元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=128)\n",
    "print(lstm_cell.state_size) # 打出state_size看一下，应为128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个简单的例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新版本没有call方法，可以调用这个查看output,h1 = lstm_cell.__call__(input,h0) \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=128)\n",
    "inputs = tf.placeholder(np.float32, shape=(32,100))\n",
    "h0 = lstm_cell.zero_state(32,np.float32)\n",
    "output, h1 = lstm_cell.__call__(inputs, h0)\n",
    "\n",
    "print(h1.h)\n",
    "print(h1.c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=128)\n",
    "inputs = tf.placeholder(np.float32, shape=(32,100))\n",
    "h0 = lstm_cell.zero_state(32,np.float32)\n",
    "output, h1 = lstm_cell(inputs, h0)\n",
    "\n",
    "print(h1.h)\n",
    "print(h1.c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对RNN进行堆叠——MultiRNNCell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多层RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 128, 128)\n",
      "(<tf.Tensor 'multi_rnn_cell/cell_0/basic_rnn_cell/Tanh:0' shape=(32, 128) dtype=float32>, <tf.Tensor 'multi_rnn_cell/cell_1/basic_rnn_cell/Tanh:0' shape=(32, 128) dtype=float32>, <tf.Tensor 'multi_rnn_cell/cell_2/basic_rnn_cell/Tanh:0' shape=(32, 128) dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 每调用一次这个函数返回一个BasicRNNCell\n",
    "def get_a_cell():\n",
    "    return tf.nn.rnn_cell.BasicRNNCell(num_units=128)\n",
    "\n",
    "# 用 tf.nn.rnn_cell.MultiRNNCell 创建3层RNN\n",
    "cell = tf.nn.rnn_cell.MultiRNNCell( [get_a_cell() for _ in range(3)] )\n",
    "\n",
    "# 得到的cell实际也是 RNNCell的子类\n",
    "# (128, 128, 128) 不是128x128x128的意思，而是表示共有3个隐层状态，每个隐层状态的大小为128\n",
    "print(cell.state_size)\n",
    "\n",
    "# 使用对应的 call 函数\n",
    "inputs = tf.placeholder(np.float32, shape=(32,100))\n",
    "h0 = cell.zero_state(32,np.float32)\n",
    "output, h1 = cell.__call__(inputs, h0)\n",
    "print(h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用tf.nn.dynamic_rnn展开时间维度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于单个的RNNCell，使用它的call函数进行运算时，只是在序列时间上前进了一步。如使用x1, h0 得到h1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果序列长度为n，要调用n次call函数，比较麻烦。因此，用dynamic实现\n",
    "通过{h1, x1, x2, ..., xn} 直接得到 { h1, h2, ..., hn }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 给出语句结构，不是直接运行\n",
    "# inputs shape = ( batch_size, time_steps, input_size )\n",
    "# cell: RNNCell\n",
    "# initial_state: shape = ( batch_size, cell.state_size )  初始状态，一般可以取零矩阵\n",
    "outputs, state = tf.nn.dynamic_rnn(cell, inputs, initial_state=initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学习使用Tensorflow实现CharRNN实现（未完成）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行环境为Python2.7以上，Tensorflow1.2以上，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "《model.py》\n",
    "self.inputs 是外部传入的一个batch内的输入数据，它的形状为( self.num_seqs, self.num_steps )，其\n",
    "            中self.num_seqs表示一个batch内句子的个数， self.num_steps 表示每个句子的长度\n",
    "self.targets 是 self.inputs 对应的训练目标，它的形状和 self.inputs相同，内容是 self.inputs 每个字母对应的下             一个字母。\n",
    "self.keep_prob 是因为后面的模型有Dropout层，控制Dropout层所需要的概率。训练时，使用 self.keep_prob=0.5，\n",
    "            测试时，使用 self.keep_prob=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inputs(self):\n",
    "    with tf.name_scope('inputs'):\n",
    "        self.inputs = tf.placeholder(tf.int32, shape=(\n",
    "            self.num_seqs, self.num_steps), name='inputs')\n",
    "        self.targets = tf.placeholder(tf.int32, shape=(\n",
    "            self.num_seqs, self.num_steps), name='targets')\n",
    "        self.keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "        # 对于中文，需要使用embedding层\n",
    "        # 英文字母没有必要用embedding层\n",
    "        if self.use_embedding is False:\n",
    "            self.lstm_inputs = tf.one_hot(self.inputs, self.num_classes)\n",
    "        else:\n",
    "            with tf.device(\"/cpu:0\"):\n",
    "                embedding = tf.get_variable('embedding', [self.num_classes, self.embedding_size])\n",
    "                self.lstm_inputs = tf.nn.embedding_lookup(embedding, self.inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的函数定义了多层的 N V N LSTM模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm(self):\n",
    "    # 创建单个cell并堆叠多层\n",
    "    def get_a_cell(lstm_size, keep_prob):\n",
    "        lstm = tf.nn.rnn_cell.BasicLSTMCell(lstm_size)\n",
    "        drop = tf.nn.rnn_cell.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        return drop\n",
    "\n",
    "    with tf.name_scope('lstm'):\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell(\n",
    "            [get_a_cell(self.lstm_size, self.keep_prob) for _ in range(self.num_layers)]\n",
    "        )\n",
    "        self.initial_state = cell.zero_state(self.num_seqs, tf.float32)\n",
    "\n",
    "        # 通过dynamic_rnn对cell展开时间维度\n",
    "        self.lstm_outputs, self.final_state = tf.nn.dynamic_rnn(cell, self.lstm_inputs, initial_state=self.initial_state)\n",
    "\n",
    "        # 通过lstm_outputs得到概率\n",
    "        seq_output = tf.concat(self.lstm_outputs, 1)\n",
    "        x = tf.reshape(seq_output, [-1, self.lstm_size])\n",
    "\n",
    "        with tf.variable_scope('softmax'):\n",
    "            softmax_w = tf.Variable(tf.truncated_normal([self.lstm_size, self.num_classes], stddev=0.1))\n",
    "            softmax_b = tf.Variable(tf.zeros(self.num_classes))\n",
    "\n",
    "        self.logits = tf.matmul(x, softmax_w) + softmax_b\n",
    "        self.proba_prediction = tf.nn.softmax(self.logits, name='predictions')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loss(self):\n",
    "    with tf.name_scope('loss'):\n",
    "        y_one_hot = tf.one_hot(self.targets, self.num_classes)\n",
    "        y_reshaped = tf.reshape(y_one_hot, self.logits.get_shape())\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=y_reshaped)\n",
    "        self.loss = tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上是模型从输入到损失的全过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "《 训练模型与生成文字过程 》"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先来看简单的生成英文的例子。使用的训练文件 shakespeare.txt 放置在data/文件夹下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "训练命令为\n",
    "python train.py \\\n",
    "    --input_file data/shakespeare.txt \\\n",
    "    --name shakespeare \\\n",
    "    --num_steps 50 \\\n",
    "    --num_seqs 32 \\\n",
    "    --learning_rate 0.01 \\\n",
    "    --max_steps 20000 \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
